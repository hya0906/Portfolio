import librosa, soundfile
import numpy as np
import glob
import os
import pandas as pd
from tensorflow.python.client import device_lib
import tensorflow as tf
import matplotlib.pyplot as plt
import random
import librosa.display as d
import numpy as np
from datasets import load_dataset
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
from tqdm import tqdm
from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ['CUDA_VISIBLE_DEVICES'] = "0"

print(device_lib.list_local_devices())
print(tf.test.is_built_with_cuda())
tf.config.list_physical_devices('GPU')
#tf.test.is_gpu_available('GPU') 위의 함수로 바뀐다고 함 (2.4.0)
print(tf.sysconfig.get_build_info())

# https://www.kaggle.com/code/kartik2khandelwal/data-augmentation

# NOISE
def noise(data, noise_factor=0.005):
    wn = np.random.randn(len(data))
    augmented_data = data + noise_factor * wn
    augmented_data = augmented_data.astype(type(data[0]))
    return augmented_data

# SHIFT
def shift(data, shift_range):
    return np.roll(data, shift_range)
# PITCH
def pitch(data, sampling_rate, pitch_factor=0.7):
    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)

def scaling_amplitude(data, amplitude_factor=5):
    data *= amplitude_factor
    return data

audio_aug = Compose([
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=0.5),
    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),
    Shift(min_shift = -0.3, max_shift = 0.3, rollover=True, p=0.5),
])

ll = []
def get_feature(file_path, sr, mfcc_len=39, mean_signal_length=90000, test = False):
    """
    file_path: Speech signal folder
    mfcc_len: MFCC coefficient length
    mean_signal_length: MFCC feature average length
  	"""
    signal, fs = librosa.load(file_path, sr=sr)
    s_len = len(signal)
    ll.append(s_len)


    if test is False:
        if np.random.randint(2) == 0:
            signal = scaling_amplitude(signal, amplitude_factor=random.uniform(0.8, 1.2))
        signal = audio_aug(signal, sample_rate=16000)
        # print("augmentation")

    if s_len < mean_signal_length:
        signal = np.tile(signal, int(mean_signal_length//s_len)+1)
        # d.waveshow(y=signal, sr=fs)  # 소리 데이터를 그래프로 표현하기
        # plt.title('Waveplot')  # 그래프 제목 설정하기
        # plt.show()

        pos = random.randint(0, ((mean_signal_length//s_len)+1) * (s_len) - mean_signal_length)
        # print((int(mean_signal_length//s_len)+1) * (s_len))
        # print(0, (int(mean_signal_length//s_len)+1) * (s_len) - mean_signal_length, "pos", pos)
        # print(signal[pos:pos + s_len] , signal[pos:pos + mean_signal_length].shape)
        signal = signal[pos:pos + mean_signal_length]
        # d.waveshow(y=signal, sr=fs)  # 소리 데이터를 그래프로 표현하기
        # plt.title('Waveplot')  # 그래프 제목 설정하기
        # plt.show()
    else:
        pad_len = s_len - mean_signal_length
        pad_len //= 2
        signal = signal[pad_len:pad_len + mean_signal_length]

    #mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=mfcc_len).T
    print(np.array(signal).shape)
    return signal #mfcc


def prepare_CREMA_DS(path_audios):

    dict_emotions_crema = {"NEU": 0,
                           "HAP": 1,
                           "SAD": 2,
                           "ANG": 3,
                           "FEA": 4,
                           "DIS": 5}
    data = []
    for path in tqdm(Path(path_audios).glob("**/*.wav")):
        name = str(path).split('\\')[-1].split('.')[0]

        label = name.split("_")[2]
        actor = int(name.split("_")[0])

        try:
            data.append({
                "name": name,
                "path": path,
                "emotion": label,
                "actor": actor
            })
        except Exception as e:
            # print(str(path), e)
            pass
    df = pd.DataFrame(data)
    return df

def generate_train_test(fold, df, save_path=""):
    """
    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training
    of each fold.

    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]
    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function
    :param save_path:[str] Path to save the train.csv and test.csv per fold
    """
    actors_per_fold = {
        0: [1090, 1059, 1086, 1063, 1064, 1065, 1067, 1069, 1008, 1012, 1044, 1076, 1015, 1077, 1082, 1020, 1021, 1054], #1469
        1: [1024, 1057, 1091, 1030, 1062, 1051, 1034, 1003, 1004, 1038, 1007, 1070, 1009, 1041, 1043, 1013, 1019, 1022], #1464
        2: [1088, 1089, 1026, 1001, 1035, 1005, 1006, 1081, 1085, 1014, 1079, 1016, 1049, 1018, 1083, 1084, 1053, 1055], #1476
        3: [1025, 1027, 1029, 1061, 1031, 1032, 1002, 1066, 1037, 1040, 1072, 1010, 1042, 1074, 1078, 1080, 1017, 1087], #1475
        4: [1028, 1033, 1036, 1039, 1045, 1046, 1047, 1048, 1050, 1052, 1056, 1058, 1060, 1068, 1071, 1073, 1075, 1011, 1023], #1558
    }

    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]
    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]

    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    if(save_path!=""):
        train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
        test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
    return train_df, test_df



#RAVDE_CLASS_LABELS = ("angry", "calm", "disgust", "fear", "happy", "neutral","sad","surprise")#rav
CREMA_CLASS_LABELS = ("angry", "disgust", "fear", "happy", "neutral","sad")
#01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised

dict_emotions_crema = {"NEU": 0,
                           "HAP": 1,
                           "SAD": 2,
                           "ANG": 3,
                           "FEA": 4,
                           "DIS": 5}

#RAVDESS
with tf.device('/device:GPU:1'):
    for fold in range(5):
        sr = 16000
        length = 58726 #84351
        num = 39
        PATH = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\CREMA-D"
        save_path = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\distillation_231006\\FineTuningWav2Vec2_out"
        WAV_PATH = os.listdir(PATH)
        ls = glob.glob(PATH)

        #os.makedirs(f'./new_data/total_CREMA_aug_wav_{sr}_{length}_padding_x5', exist_ok=True)
        now = datetime.now()
        now_time = datetime.strftime(now, '%Y-%m-%d_%H-%M-%S')

        df = prepare_CREMA_DS(PATH)
        print("df", df)
        _, _ = generate_train_test(0, df, save_path)
        data_files = {
            "train": os.path.join(save_path, "train.csv"),
            "validation": os.path.join(save_path, "test.csv"),
        }

        # Load data
        dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
        train_dataset = dataset["train"]
        eval_dataset = dataset["validation"]

        speech_list = [path for path in train_dataset["path"]]
        print(len(speech_list))
        print(train_dataset)
        print(eval_dataset)

        # Augment train data
        features = []
        labels = []
        for idx, data in enumerate(tqdm(train_dataset)):
            # 원본1개 + 변형 7개
            for a in range(5):
                # label = [0 for i in range(len(CREMA_CLASS_LABELS))]
                # label[dict_emotions_crema[data["emotion"]]] = float(1)
                # labels.append(label)

                if a == 0:
                    feature = get_feature(data["path"], sr=sr, mean_signal_length=length, test=True)
                    # print("no aug - idx: ", idx, len(feature))
                else:
                    feature = get_feature(data["path"], sr=sr, mean_signal_length=length)
                    # print("aug - idx: ", idx)
                features.append(feature)
                soundfile.write(f'.\\new_data\\total_CREMA_aug_wav_{sr}_{length}_padding_x5_append\\{data["name"]}_{a}.wav',
                                feature,
                                16000,
                                format='WAV')
            if (idx + 1) % 500 == 0:
                print("idx: ", idx + 1)
                # print(np.array(features).shape)

        # features = np.array(features, dtype="float32")
        # labels = np.array(labels, dtype="float32")
        # print(features.shape, labels.shape)
        #
        # train_data = {"x": features, "y": labels}
        #
        # name1 = "aug_wav_train_data_90000"
        # np.save(f'./new_data/{name1}_{now_time}', train_data)
        # x_save_load = np.load(f'./new_data/{name1}_{now_time}.npy', allow_pickle=True)
        # print(x_save_load)
        # print("train done")

        #################################
        # Augment test data
        speech_list = [path for path in eval_dataset["path"]]
        print(len(speech_list))

        features = []
        labels = []
        for idx, data in enumerate(tqdm(eval_dataset)):
            # label = [0 for i in range(len(CREMA_CLASS_LABELS))]
            # label[dict_emotions_crema[data["emotion"]]] = float(1)
            # labels.append(label)

            feature = get_feature(data["path"], sr=sr, mean_signal_length=length, test=True)
            features.append(feature)
            soundfile.write(f".\\new_data\\total_CREMA_aug_wav_{sr}_{length}_padding_x5_append\\{data['name']}_{a}.wav",
                            feature,
                            16000,
                            format='WAV')
            if (idx + 1) % 100 == 0:
                print("idx: ", idx + 1)
                # print(np.array(features).shape)

        # features = np.array(features, dtype="float32")
        # labels = np.array(labels, dtype="float32")
        # print(features.shape, labels.shape)
        #
        # test_data = {"x": features, "y": labels}
        #
        # name2 = "aug_wav_test_data_90000"
        # np.save(f'./new_data/{name2}_{now_time}', test_data)
        # x_save_load = np.load(f'./new_data/{name2}_{now_time}.npy', allow_pickle=True)
        # print(x_save_load)
        # print("test done")
        if fold == 0:
            break

