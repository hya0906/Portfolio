import random
import logging
#from optimum.onnxruntime import WAV2VEC
#from keras_flops import get_flops

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_hub as hub
import pandas as pd
from tensorflow.keras import callbacks
from datetime import datetime
#import datetime
from wav2vec2 import Wav2Vec2Config
from datasets import load_dataset
from sklearn.metrics import confusion_matrix
from transformers import AutoFeatureExtractor, TFWav2Vec2Model
import librosa
import time
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
import os
from sklearn.metrics import f1_score
from sklearn.metrics import ConfusionMatrixDisplay
from datasets import load_from_disk
import sys
# import tf2onnx as tt
# import onnxruntime
# import onnx
from transformers.onnx import export
tf.keras.backend.clear_session()

# Only log error messages
tf.get_logger().setLevel(logging.ERROR)
# Set random seed
tf.keras.utils.set_random_seed(42)

# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.
MAX_DURATION = 1
# Sampling rate is the number of samples of audio recorded every second
SAMPLING_RATE = 16000
BATCH_SIZE = 2 #32  # Batch-size for training and evaluating our model.
NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).
HIDDEN_DIM = 768 #1024  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).
MAX_SEQ_LENGTH = 84351 #60000  # Maximum length of the input audio file.
# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.
MAX_FRAMES = 187 #(60000) #249 #(80000)
MAX_EPOCHS = 8  # Maximum number of training epochs.
WAV_DATA_POINTS = 90000 #110000

MODEL_CHECKPOINT = "facebook/wav2vec2-base"  # Name of pretrained model from Hugging Face Model Hub

LEARNING_RATE = 4e-6
config = Wav2Vec2Config()



def prepare_RAVDESS_DS(path_audios):
    """
    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:
     ______________________________________________________________________________________________________________________________
    |             name            |                     path                                   |     emotion      |     actor     |
    ______________________________________________________________________________________________________________________________
    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |
    ______________________________________________________________________________________________________________________________
    ...

    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)
    """
    dict_emotions_ravdess = {
        0: 'Neutral',
        1: 'Happy',
        2: 'Sad',
        3: 'Angry',
        4: 'Fear',
        5: 'Disgust',
        6: 'Surprise'
    }
    data = []
    for path in tqdm(Path(path_audios).glob("**/*.wav")):
        name = str(path).split('\\')[-1].split('.')[0]
        if int(name.split("-")[2]) == 1:
            label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]
        else:
            if int(name.split("-")[2]) == 2:
                label = dict_emotions_ravdess[0]
            elif int(name.split("-")[2]) != 2:
                label = dict_emotions_ravdess[int(name.split("-")[2]) - 2]
        actor = int(name.split("-")[-2])

        try:
            data.append({
                "name": name,
                "path": path,
                "emotion": label,
                "actor": actor
            })
        except Exception as e:
            # print(str(path), e)
            pass
    df = pd.DataFrame(data)
    return df

def generate_train_test(fold, df, save_path=""):
    """
    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training
    of each fold.

    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]
    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function
    :param save_path:[str] Path to save the train.csv and test.csv per fold
    """
    actors_per_fold = {
        0: [2, 5, 14, 15, 16],
        1: [3, 6, 7, 13, 18],
        2: [10, 11, 12, 19, 20],
        3: [8, 17, 21, 23, 24],
        4: [1, 4, 9, 22],
    }

    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]
    #test_df = test_df.head(1) # Inference time 측정할때 사용

    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]

    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    if(save_path!=""):
        train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
        test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
    return train_df, test_df

def speech_file_to_array_fn(path):
    """
    Loader of audio recordings. It loads the recordings and convert them to a specific sampling rate if required, and returns
    an array with the samples of the audio.

    :param path:[str] Path to the wav file.
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_array, sampling_rate = librosa.load(path)
    speech = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)
    return speech


feature_extractor = AutoFeatureExtractor.from_pretrained(
    MODEL_CHECKPOINT, return_attention_mask=True
)
def preprocess_function(examples, input_column = "path", output_column = "emotion"):
    """
    Load the recordings with their labels.

    :param examples:[DataFrame]  with the samples of the training or test sets.
    :param input_column:[str]  Column that contain the paths to the recordings
    :param output_column:[str]  Column that contain the emotion associated to each recording
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]] #Confusion matrix 그릴때 사용
    #speech_list = [speech_file_to_array_fn(examples[input_column])] #Inference time 측정할때 사용

    target_list = [label_to_id(label, label_list) for label in examples[output_column]]
    #print("speech_list!!", np.array(speech_list).shape, np.array(speech_list[0]).shape)

    result = feature_extractor(
        speech_list,
        sampling_rate=feature_extractor.sampling_rate,
        max_length= MAX_SEQ_LENGTH,
        truncation=True,
        padding="max_length",
    )

    result["labels"] = list(map(int, target_list))
    return result

def label_to_id(label, label_list):

    if len(label_list) > 0:
        return label_list.index(label) if label in label_list else -1

    return label


def mean_pool(hidden_states, feature_lengths):
    attenion_mask = tf.sequence_mask(
        feature_lengths, maxlen= None, dtype=tf.dtypes.int64
    )
    padding_mask = tf.cast(
        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),
        dtype=tf.dtypes.bool,
    )
    hidden_states = tf.where(
        tf.broadcast_to(
            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)
        ),
        0.0,
        hidden_states,
    )
    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(
        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),
        [-1, 1],
    )
    return pooled_state


class TFWav2Vec2ForAudioClassification(layers.Layer):
    """Combines the encoder and decoder into an end-to-end model for training."""

    def __init__(self, model_checkpoint):
        super().__init__()
        # Instantiate the Wav2Vec 2.0 model without the Classification-Head
        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(
            model_checkpoint, apply_spec_augment=False, from_pt=True
        )

        self.pooling = layers.GlobalAveragePooling1D()
        # Drop-out layer before the final Classification-Head
        self.intermediate_layer_dropout = layers.Dropout(0.1)
        self.middle_layer = layers.Dense(HIDDEN_DIM)
        # Classification-Head
        self.final_layer = layers.Dense(8, activation="softmax")

    def call(self, inputs):
        # We take only the first output in the returned dictionary corresponding to the
        # output of the last layer of Wav2vec 2.0
        hidden_states = self.wav2vec2(inputs["input_values"])[0]

        # # If attention mask does exist then mean-pool only un-masked output frames
        # if tf.is_tensor(inputs["attention_mask"]):
        #     # Get the length of each audio input by summing up the attention_mask
        #     # (attention_mask = (BATCH_SIZE x MAX_SEQ_LENGTH) ∈ {1,0})
        #     audio_lengths = tf.cumsum(inputs["attention_mask"], -1)[:, -1]
        #     # Get the number of Wav2Vec 2.0 output frames for each corresponding audio input
        #     # length
        #     feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(
        #         audio_lengths
        #     )
        #     pooled_state = mean_pool(hidden_states, feature_lengths)
        # # If attention mask does not exist then mean-pool only all output frames
        # else:
        #     pooled_state = self.pooling(hidden_states)
        pooled_state = self.pooling(hidden_states)
        return pooled_state #final_state

def build_model():
    # Model's input
    inputs = {
        "input_values": tf.keras.Input(shape=(None,), dtype="float32"),
        "attention_mask": tf.keras.Input(shape=(None,), dtype="int32"),
    }
    # Instantiate the Wav2Vec 2.0 model with Classification-Head using the desired
    # pre-trained checkpoint
    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT)(
        inputs
    )
    x = layers.Dropout(0.1)(wav2vec2_model)
    x = layers.Dense(HIDDEN_DIM)(x)

    #x = layers.Dense(HIDDEN_DIM)(x)
    # Classification-Head
    x = layers.Dense(7, activation="softmax")(x)

    # Model
    model = tf.keras.Model(inputs=inputs, outputs=x)

    # Loss
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    # Optimizer
    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    # Compile and return
    model.compile(loss=loss, optimizer=optimizer, metrics=["accuracy"])
    return model


def make_tf2onnx(model, save_path):
    onnx_model, _ = tt.convert.from_keras(
        model, input_signature=[{"attention_mask": tf.TensorSpec(shape=[None, 84351], dtype=tf.float32),
                                 "input_values": tf.TensorSpec(shape=[None, 84351], dtype=tf.float32)}]
    )
    onnx.save(onnx_model, save_path)
    print("save onnx model!!")


def batch1_run_onnx(path): #CPU inference time은 안됨.
    session = onnxruntime.InferenceSession(path)
    session.set_providers(['CUDAExecutionProvider']) #CUDAExecutionProvider / CPUExecutionProvider
    print(onnxruntime.get_device())

    input1 = test_x["attention_mask"].reshape(1, 84351).astype(np.float32)
    input2 = test_x["input_values"].reshape(1, 84351).astype(np.float32)

    now = time.time()
    ort_outs = session.run(output_names=None, input_feed={"args_0/attention_mask": input1,
                                                          "args_0/input_values": input2})
    a = time.time() - now
    print("Inference time: ", a, "sec")


def batch1_run_tfmodel(model, test_x):
    test_x["attention_mask"] = test_x["attention_mask"].reshape(1, 84351)
    test_x["input_values"] = test_x["input_values"].reshape(1, 84351)

    now = time.time()
    y_pred = model.predict(test_x)
    a = time.time() - now
    print("Inference time: ", a, "sec")


def f1score_confusion_matrix(model, test_x, label):
    y_pred = model.predict(test_x, batch_size= 2)
    print("f1 score: ", f1_score(label, np.argmax(y_pred, axis=1), average='weighted'))
    cm = confusion_matrix(label, np.argmax(y_pred, axis=1), normalize="true")
    disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                                  display_labels=['Neutral', 'Happy', 'Sad', 'Angry', 'Fear', 'Disgust', 'Surprise'])
    disp.plot(cmap=plt.cm.Blues)
    plt.show()
    # plt.savefig('.\\Models\\RAVDESS_RAVDESS_aug_wavfile_16000_padding84351_2023-10-10_10-41-54\\confusion_matrix_0.84.png')



#https://github.com/huggingface/transformers/issues/16249 TFtrainer이제 사용못함.
#audios_dir = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\백업파일\\백업\\dataset\\speech\\RAVDESS\\audios_16kHz\\total"
data_name = "RAVDESS_aug_wavfile_16000_padding84351"
#data_name = "RAVDESS"
#audios_dir = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\TIM-Net_SER-main\\RAVDESS - 복사본"
audios_dir = f"C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\new_data\\{data_name}"
data_path = 'C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\FineTuningWav2Vec2_out'
cache_dir = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\cache_dir"
if __name__ == "__main__":
    start = time.time()
    now = datetime.now()
    current_time = now.strftime("%Y%m%d_%H%M%S")
    out_dir_models = os.path.join(data_path, "trained_models")  # out path to save trained models
    # We need to specify the input and output column
    input_column = "path"  # Name of the column that will contain the path of the recordings
    output_column = "emotion"  # Name of the column that will contain the labels of the recordings



    with tf.device('/device:GPU:1'):
        now = datetime.now()
        now_time = datetime.strftime(now, '%Y-%m-%d_%H-%M-%S')
        for fold in range(5):  # 5-CV strategy
            print(f"====fold {fold}====")
            out_dir_models_path = os.path.join(out_dir_models, current_time, "fold" + str(fold))
            save_path = os.path.join(data_path, current_time, "fold" + str(fold))
            os.environ['TRANSFORMERS_CACHE'] = os.path.join(cache_dir, current_time, "fold" + str(fold))
            os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, current_time, "fold" + str(fold))
            # os.makedirs(save_path, exist_ok=True)
            # print("SAVING DATA IN: ", save_path)
            #
            # df = prepare_RAVDESS_DS(audios_dir)
            # print("df", df)
            # _, _ = generate_train_test(fold, df, save_path)
            #
            # #save_path = "D:\\yuna_hong\\WAV2VEC\\FineTuningWav2Vec2_out\\20230928_193309\\fold0"
            # data_files = {
            #     "train": os.path.join(save_path, "train.csv"),
            #     "validation": os.path.join(save_path, "test.csv"),
            # }
            #
            # # Load data
            # dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
            # train_dataset = dataset["train"]
            # eval_dataset = dataset["validation"]
            # print("Processing fold: ", str(fold), " - actors in Train fold: ", set(train_dataset["actor"]))
            # print("Processing fold: ", str(fold), " - actors in Eval fold: ", set(eval_dataset["actor"]))
            # label_list = train_dataset.unique(output_column)
            # label_list.sort()  # Let's sort it for determinism
            # num_labels = len(label_list)
            # print(f"A classification problem with {num_labels} classes: {label_list}")
            # print("@@",dataset)
            #
            # eval_dataset = eval_dataset.select(
            #     [i for i in range((len(eval_dataset) // 1) * 1)]
            # )
            # print(eval_dataset)
            # print("Generating test...")
            # eval_dataset = eval_dataset.map(
            #     preprocess_function,
            #     batch_size= BATCH_SIZE, #Inference time 측정할때 사용안함
            #     batched=True, #Inference time 측정할때 사용안함
            #     # num_proc=4
            #     #remove_columns=['name', 'path', 'emotion', 'actor'],
            # )

            eval_dataset = load_from_disk("./wav2vec_huggingface_dataset/test")
            test = eval_dataset.shuffle(seed=42).with_format("numpy")[:]


            print("validation dataset", type(eval_dataset["input_values"]), np.array(eval_dataset["input_values"]).shape)

            # Remove targets from training dictionaries
            test_x = {x: y for x, y in test.items() if x != "label"}


            model = build_model()
            model.summary()
            # flops = get_flops(model, batch_size=1)
            # print(f"FLOPS: {flops / 10 ** 9:.03} G")

            #weight_path = f"C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\RAVDESS_RAVDESS_aug_wavfile_16000_padding84351_7class_2023-10-15_10-12-46\\10-fold_weights_best_0_0.84000.hdf5"
            #weight_path = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\Models\\_RAVDESS_main_2023-10-27_11-08-50\\10-fold_weights_best_0_0.85000.hdf5"

            #weight_path = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\Models\\_RAVDESS_main_2023-10-27_19-33-01\\10-fold_weights_best_0_0.84000.hdf5"
            weight_path = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\Models\\_RAVDESS_main_2023-10-27_19-34-46_0.86\\10-fold_weights_best_0_0.86000.hdf5"
            model.load_weights(weight_path)

            #convert tf to onnx and run
            path = "./WAV2VEC2_0.84.onnx"
            # make_tf2onnx(model, path)
            # batch1_run_onnx(path)

            #tf model run
            # batch1_run_tfmodel(model, test_x)

            results = model.evaluate(test_x, test["labels"])
            f1score_confusion_matrix(model, test_x, test["labels"])

            if fold == 0:
                break


