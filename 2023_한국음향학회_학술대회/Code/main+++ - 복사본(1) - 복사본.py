import random
import logging

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_hub as hub
import pandas as pd
from tensorflow.keras import callbacks
from datetime import datetime
#import datetime
from wav2vec2 import Wav2Vec2Config
from datasets import load_dataset
from transformers import AutoFeatureExtractor, TFWav2Vec2Model
import librosa
import time

from pathlib import Path
from tqdm import tqdm
import os
import sys
tf.keras.backend.clear_session()

# Only log error messages
tf.get_logger().setLevel(logging.ERROR)
# Set random seed
tf.keras.utils.set_random_seed(42)

# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.
MAX_DURATION = 1
# Sampling rate is the number of samples of audio recorded every second
SAMPLING_RATE = 16000
BATCH_SIZE = 2 #32  # Batch-size for training and evaluating our model.
NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).
HIDDEN_DIM = 768 #1024  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).
MAX_SEQ_LENGTH = 80000  # Maximum length of the input audio file.
# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.
MAX_FRAMES = 249
MAX_EPOCHS = 10  # Maximum number of training epochs.
WAV_DATA_POINTS = 90000 #110000

MODEL_CHECKPOINT = "facebook/wav2vec2-base"  # Name of pretrained model from Hugging Face Model Hub
# feature_extractor = AutoFeatureExtractor.from_pretrained(
#     "facebook/wav2vec2-large-960h", return_attention_mask=True
# )
#
#
# processor = AutoProcessor.from_pretrained("facebook/wav2vec2-large-960h")
# pretrained_layer = TFWav2Vec2Model.from_pretrained("facebook/wav2vec2-large-960h", from_pt=True)
# target_sampling_rate = processor.feature_extractor.sampling_rate
# print(f"The target sampling rate: {target_sampling_rate}")

LEARNING_RATE = 5e-5
config = Wav2Vec2Config()



def prepare_RAVDESS_DS(path_audios):
    """
    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:
     ______________________________________________________________________________________________________________________________
    |             name            |                     path                                   |     emotion      |     actor     |
    ______________________________________________________________________________________________________________________________
    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |
    ______________________________________________________________________________________________________________________________
    ...

    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)
    """
    dict_emotions_ravdess = {
        0: 'Neutral',
        1: 'Calm',
        2: 'Happy',
        3: 'Sad',
        4: 'Angry',
        5: 'Fear',
        6: 'Disgust',
        7: 'Surprise'
    }
    data = []
    for path in tqdm(Path(path_audios).glob("**/*.wav")):
        if int(str(path).split('\\')[-1].split('.')[0].split("-")[-1]) in [2, 5, 14, 15, 16]:
            name = str(path).split('\\')[-1].split('.')[0]
            label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]  # Start emotions in 0
            actor = int(name.split("-")[-1])

            try:
                data.append({
                    "name": name,
                    "path": path,
                    "emotion": label,
                    "actor": actor
                })
            except Exception as e:
                # print(str(path), e)
                pass

        else:
            for i in range(8):
                name = str(path).split('\\')[-1].split('.')[0]
                label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]  # Start emotions in 0
                actor = int(name.split("-")[-1])

                try:
                    data.append({
                        "name": name,
                        "path": path,
                        "emotion": label,
                        "actor": actor
                    })
                except Exception as e:
                    # print(str(path), e)
                    pass

    df = pd.DataFrame(data)
    return df

def generate_train_test(fold, df, save_path=""):
    """
    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training
    of each fold.

    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]
    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function
    :param save_path:[str] Path to save the train.csv and test.csv per fold
    """
    actors_per_fold = {
        0: [2, 5, 14, 15, 16],
        1: [3, 6, 7, 13, 18],
        2: [10, 11, 12, 19, 20],
        3: [8, 17, 21, 23, 24],
        4: [1, 4, 9, 22],
    }

    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]
    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]

    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    if(save_path!=""):
        train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
        test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
    return train_df, test_df

def speech_file_to_array_fn(path):
    """
    Loader of audio recordings. It loads the recordings and convert them to a specific sampling rate if required, and returns
    an array with the samples of the audio.

    :param path:[str] Path to the wav file.
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_array, sampling_rate = librosa.load(path)
    speech = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)
    return speech


feature_extractor = AutoFeatureExtractor.from_pretrained(
    MODEL_CHECKPOINT, return_attention_mask=True
)
def preprocess_function(examples, input_column = "path", output_column = "emotion"):
    """
    Load the recordings with their labels.

    :param examples:[DataFrame]  with the samples of the training or test sets.
    :param input_column:[str]  Column that contain the paths to the recordings
    :param output_column:[str]  Column that contain the emotion associated to each recording
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]
    target_list = [label_to_id(label, label_list) for label in examples[output_column]]
    if len(speech_list) == 8:
        speech_list = [speech_list[0]]
        target_list = [target_list[0]]
    #processor과 feature_extractor차이?? 똑같이 truncation하고 padding이랑 sampling rate맞추는건가?
    #print("speech_list!!", np.array(speech_list).shape, np.array(speech_list[0]).shape)
    result = feature_extractor(
        speech_list,
        sampling_rate=feature_extractor.sampling_rate,
        max_length=MAX_SEQ_LENGTH,
        truncation=True,
    )
    result["labels"] = list(map(int, target_list))
    return result

def label_to_id(label, label_list):

    if len(label_list) > 0:
        return label_list.index(label) if label in label_list else -1

    return label



#################################3


def mean_pool(hidden_states, feature_lengths):
    attenion_mask = tf.sequence_mask(
        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64
    )
    padding_mask = tf.cast(
        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),
        dtype=tf.dtypes.bool,
    )
    hidden_states = tf.where(
        tf.broadcast_to(
            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)
        ),
        0.0,
        hidden_states,
    )
    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(
        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),
        [-1, 1],
    )
    return pooled_state


class TFWav2Vec2ForAudioClassification(layers.Layer):
    """Combines the encoder and decoder into an end-to-end model for training."""

    def __init__(self, model_checkpoint, num_classes):
        super().__init__()
        # Instantiate the Wav2Vec 2.0 model without the Classification-Head
        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(
            model_checkpoint, apply_spec_augment=False, from_pt=True
        )
        #self.wav2vec2.wav2vec2.feature_extractor._freeze_parameters()
        #self.wav2vec2.feature_extractor._freeze_parameters()
        self.pooling = layers.GlobalAveragePooling1D()
        # Drop-out layer before the final Classification-Head
        self.intermediate_layer_dropout = layers.Dropout(0.1)
        self.middle_layer = layers.Dense(HIDDEN_DIM)
        # Classification-Head
        self.final_layer = layers.Dense(num_classes, activation="softmax")

    def call(self, inputs):
        # We take only the first output in the returned dictionary corresponding to the
        # output of the last layer of Wav2vec 2.0
        hidden_states = self.wav2vec2(inputs["input_values"])[0]

        # If attention mask does exist then mean-pool only un-masked output frames
        if tf.is_tensor(inputs["attention_mask"]):
            # Get the length of each audio input by summing up the attention_mask
            # (attention_mask = (BATCH_SIZE x MAX_SEQ_LENGTH) ∈ {1,0})
            audio_lengths = tf.cumsum(inputs["attention_mask"], -1)[:, -1]
            # Get the number of Wav2Vec 2.0 output frames for each corresponding audio input
            # length
            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(
                audio_lengths
            )
            pooled_state = mean_pool(hidden_states, feature_lengths)
        # If attention mask does not exist then mean-pool only all output frames
        else:
            pooled_state = self.pooling(hidden_states)

        # middle_state = self.middle_layer(pooled_state)
        # intermediate_state = self.intermediate_layer_dropout(middle_state)
        # final_state = self.final_layer(intermediate_state)

        return pooled_state #final_state



def build_model():
    # Model's input
    inputs = {
        "input_values": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype="float32"),
        "attention_mask": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype="int32"),
    }
    # Instantiate the Wav2Vec 2.0 model with Classification-Head using the desired
    # pre-trained checkpoint
    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(
        inputs
    )
    x = layers.Dropout(0.1)(wav2vec2_model)
    x = layers.Dense(HIDDEN_DIM)(x)
    # Classification-Head
    x = layers.Dense(8, activation="softmax")(x)

    # Model
    model = tf.keras.Model(inputs=inputs, outputs=x)

    # Loss
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    # Optimizer
    optimizer = keras.optimizers.Adam(learning_rate=1e-5)
    # Compile and return
    model.compile(loss=loss, optimizer=optimizer, metrics=["accuracy"])
    return model

#https://github.com/huggingface/transformers/issues/16249 TFtrainer이제 사용못함.
audios_dir = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\백업파일\\백업\\dataset\\speech\\RAVDESS\\audios_16kHz\\total"
data_path = 'C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\FineTuningWav2Vec2_out'
cache_dir = "C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\cache_dir"
if __name__ == "__main__":
    start = time.time()
    now = datetime.now()
    current_time = now.strftime("%Y%m%d_%H%M%S")
    out_dir_models = os.path.join(data_path, "trained_models")  # out path to save trained models
    # We need to specify the input and output column
    input_column = "path"  # Name of the column that will contain the path of the recordings
    output_column = "emotion"  # Name of the column that will contain the labels of the recordings



    with tf.device('/device:GPU:1'):
        now = datetime.now()
        now_time = datetime.strftime(now, '%Y-%m-%d_%H-%M-%S')
        for fold in range(5):  # 5-CV strategy
            print(f"====fold {fold}====")
            out_dir_models_path = os.path.join(out_dir_models, current_time, "fold" + str(fold))
            save_path = os.path.join(data_path, current_time, "fold" + str(fold))
            os.environ['TRANSFORMERS_CACHE'] = os.path.join(cache_dir, current_time, "fold" + str(fold))
            os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, current_time, "fold" + str(fold))
            os.makedirs(save_path, exist_ok=True)
            print("SAVING DATA IN: ", save_path)

            df = prepare_RAVDESS_DS(audios_dir)
            print("df", df)
            _, _ = generate_train_test(fold, df, save_path)

            #save_path = "D:\\yuna_hong\\WAV2VEC\\FineTuningWav2Vec2_out\\20230928_193309\\fold0"
            data_files = {
                "train": os.path.join(save_path, "train.csv"),
                "validation": os.path.join(save_path, "test.csv"),
            }

            # Load data
            dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
            train_dataset = dataset["train"]
            eval_dataset = dataset["validation"]
            print("Processing fold: ", str(fold), " - actors in Train fold: ", set(train_dataset["actor"]))
            print("Processing fold: ", str(fold), " - actors in Eval fold: ", set(eval_dataset["actor"]))
            label_list = train_dataset.unique(output_column)
            label_list.sort()  # Let's sort it for determinism
            num_labels = len(label_list)
            print(f"A classification problem with {num_labels} classes: {label_list}")
            print("@@",dataset)

            print(type(train_dataset))

            train = np.load('../new_data/aug_wav_train_data_2023-10-07_16-37-37.npy', allow_pickle=True).item()
            train_features, train_labels = train["x"], train["y"]
            test = np.load('../new_data/aug_wav_test_data_2023-10-07_16-37-37.npy', allow_pickle=True).item()
            test_features, test_labels = train["x"], train["y"]
            print(train_features.shape, test_features.shape)

            train_dataset.features["inputs"] = train_features
            print(train_dataset)

            train_dataset = train_dataset.select(
                [i for i in range((len(train_dataset) // BATCH_SIZE) * BATCH_SIZE)]
            )
            eval_dataset = eval_dataset.select(
                [i for i in range((len(eval_dataset) // BATCH_SIZE) * BATCH_SIZE)]
            )

            #########
            print("Generating training...")
            train_dataset = train_dataset.map(
                preprocess_function,
                batch_size=100,
                batched=True,
                # num_proc=4,
                #remove_columns = ["name", "path", "emotion", "actor"]
            )
            print("train dataset", train_dataset)

            a = pd.DataFrame(train_dataset)
            a.to_csv(f"{save_path}/train_all.csv", sep="\t", encoding="utf-8", index=False)
            print("Generating test...")
            eval_dataset = eval_dataset.map(
                preprocess_function,
                batch_size=100,
                batched=True,
                # num_proc=4
            )
            print(eval_dataset)
            b = pd.DataFrame(eval_dataset)
            b.to_csv(f"{save_path}/train_all.csv", sep="\t", encoding="utf-8", index=False)
            print(b)
            # train = train_dataset.shuffle(seed=42).with_format("numpy")[:]
            # test = eval_dataset.shuffle(seed=42).with_format("numpy")[:]
            # print("validation dataset", type(eval_dataset["input_values"]), np.array(eval_dataset["input_values"]).shape)
            #
            # model = build_model()
            # model.summary()
            #
            # # Freeze Wav2Vec2 layer
            # for _layer in model.layers:
            #     if _layer.name == 'tf_wav2_vec2_for_audio_classification':
            #         print(f"Freezing model layer {_layer.name}")
            #         _layer.trainable = False
            #     print(_layer.name)
            #     print(_layer.trainable)
            #
            # print(train)
            # # Remove targets from training dictionaries
            # train_x = {x: y for x, y in train.items() if x != "label"}
            # test_x = {x: y for x, y in test.items() if x != "label"}
            #
            # print(train_x)
            #
            # filepath="./Models"
            # folder_address = filepath + "_RAVDESS_main_" + now_time
            # if not os.path.exists(folder_address):
            #     os.mkdir(folder_address)
            # weight_path=folder_address+'/Models/10-fold_weights_best_'+str(fold)+f"_{{val_accuracy:.5f}}.hdf5"
            # checkpoint = callbacks.ModelCheckpoint(weight_path, monitor='val_accuracy', verbose=1,save_weights_only=True,save_best_only=True,mode='max')
            #
            # model.fit(
            #     train_x,
            #     train["labels"],
            #     validation_data=(test_x, test["labels"]),
            #     batch_size=BATCH_SIZE,
            #     epochs=MAX_EPOCHS,
            #     callbacks = [checkpoint],
            # )
            # weight_path = f"C:\\Users\\711_2\\Desktop\\Yuna_Hong\\speech_expression\\WAV2MFCC\\WAV2VEC\\Models_RAVDESS_main_2023-10-06_15-54-41\\10-fold_weights_best_0_0.63333.hdf5"
            # model.load_weights(weight_path)
            if fold==0:
                break


