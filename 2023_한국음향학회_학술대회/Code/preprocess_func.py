import tensorflow as tf
import numpy as np
import pandas as pd
from tqdm import tqdm
from pathlib import Path
import librosa
from transformers import AutoFeatureExtractor
import random
import sys
sys.path.append('.')
sys.path.append('..')
sys.path.append('../../')
sys.path.append('../../../')

# Set random seed
tf.keras.utils.set_random_seed(42)

# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.
MAX_DURATION = 1
# Sampling rate is the number of samples of audio recorded every second
SAMPLING_RATE = 16000
BATCH_SIZE = 2 #32  # Batch-size for training and evaluating our model.
NUM_CLASSES = 8  # Number of classes our dataset will have (11 in our case).
HIDDEN_DIM = 768 #1024  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).
MAX_SEQ_LENGTH = 80000  # Maximum length of the input audio file.
# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.
MAX_FRAMES = 249
MAX_EPOCHS = 10  # Maximum number of training epochs.
WAV_DATA_POINTS = 90000 #110000

MODEL_CHECKPOINT = "facebook/wav2vec2-base"  # Name of pretrained model from Hugging Face Model Hub
feature_extractor = AutoFeatureExtractor.from_pretrained(
     MODEL_CHECKPOINT, return_attention_mask=True
 )


def prepare_RAVDESS_DS(path_audios):
    """
    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:
     ______________________________________________________________________________________________________________________________
    |             name            |                     path                                   |     emotion      |     actor     |
    ______________________________________________________________________________________________________________________________
    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |
    ______________________________________________________________________________________________________________________________
    ...

    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)
    """

    dict_emotions_ravdess = {
        0: 'Neutral',
        1: 'Happy',
        2: 'Sad',
        3: 'Angry',
        4: 'Fear',
        5: 'Disgust',
        6: 'Surprise'
    }
    print(path_audios)
    print(Path(path_audios).glob("**/*.wav"))
    data = []
    for path in tqdm(Path(path_audios).glob("**/*.wav")):
        name = str(path).split('\\')[-1].split('.')[0]
        if int(name.split("-")[2]) == 1:
            label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]
        else:
            if int(name.split("-")[2]) == 2:
                label = dict_emotions_ravdess[0]
            elif int(name.split("-")[2]) != 2:
                label = dict_emotions_ravdess[int(name.split("-")[2]) - 2]

        # label = dict_emotions_ravdess[int(name.split("-")[2]) - 1]  # Start emotions in 0
        actor = int(name.split("-")[-2])

        try:
            data.append({
                "name": name,
                "path": path,
                "emotion": label,
                "actor": actor
            })
        except Exception as e:
            # print(str(path), e)
            pass
    df = pd.DataFrame(data)
    return df

def generate_train_test(fold, df, save_path=""):
    """
    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training
    of each fold.

    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]
    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function
    :param save_path:[str] Path to save the train.csv and test.csv per fold
    """
    actors_per_fold = {
        0: [2, 5, 14, 15, 16],
        1: [3, 6, 7, 13, 18],
        2: [10, 11, 12, 19, 20],
        3: [8, 17, 21, 23, 24],
        4: [1, 4, 9, 22],
    }

    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]
    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]

    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    if(save_path!=""):
        train_df.to_csv(f"{save_path}/train.csv", sep="\t", encoding="utf-8", index=False)
        test_df.to_csv(f"{save_path}/test.csv", sep="\t", encoding="utf-8", index=False)
    return train_df, test_df

def speech_file_to_array_fn(path):
    """
    Loader of audio recordings. It loads the recordings and convert them to a specific sampling rate if required, and returns
    an array with the samples of the audio.

    :param path:[str] Path to the wav file.
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    speech_array, sampling_rate = librosa.load(path)
    speech = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=16000)
    return speech

def label_to_id(label, label_list):

    if len(label_list) > 0:
        return label_list.index(label) if label in label_list else -1

    return label

def preprocess_function(examples, input_column = "path", output_column = "emotion"):
    """
    Load the recordings with their labels.

    :param examples:[DataFrame]  with the samples of the training or test sets.
    :param input_column:[str]  Column that contain the paths to the recordings
    :param output_column:[str]  Column that contain the emotion associated to each recording
    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model
    """
    label_list = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']
    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]
    target_list = [label_to_id(label, label_list) for label in examples[output_column]]
    #processor과 feature_extractor차이?? 똑같이 truncation하고 padding이랑 sampling rate맞추는건가?
    #print("speech_list!!", np.array(speech_list).shape, np.array(speech_list[0]).shape)
    result = feature_extractor(
        speech_list,
        sampling_rate=feature_extractor.sampling_rate,
        max_length=MAX_SEQ_LENGTH,
        truncation=True,
        padding="max_length",
    )
    result["labels"] = list(map(int, target_list))
    return result

def get_mfcc_feature(file_path, mfcc_len=39, mean_signal_length=90000):
    """
    file_path: Speech signal folder
    mfcc_len: MFCC coefficient length
    mean_signal_length: MFCC feature average length
  	"""
    signal, sr = librosa.load(file_path, sr = 16000)
    #signal = librosa.resample(signal, orig_sr=sr, target_sr=16000)
    s_len = len(signal)

    pos = random.randint(0, 110000 - s_len)
    # 패칭개수 110000 - s_len - pos
    signal = np.pad(signal, (pos, 110000 - s_len - pos), 'constant', constant_values=0)
    # if s_len < mean_signal_length:
    #     pad_len = mean_signal_length - s_len
    #     pad_rem = pad_len % 2
    #     pad_len //= 2
    #     signal = np.pad(signal, (pad_len, pad_len + pad_rem), 'constant', constant_values=0)
    # else:
    #     pad_len = s_len - mean_signal_length
    #     pad_len //= 2
    #     signal = signal[pad_len:pad_len + mean_signal_length]

    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=mfcc_len)
    return mfcc.T